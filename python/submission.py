# -*- coding: utf-8 -*-
"""Submission.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RCqBADGcmpKSIuasmViG98iJEgoO7Y9A

Installing PyTorch Utilities: EfficientNet, TorchMetrics, and TorchInfo
"""

# !pip install efficientnet_pytorch
# !pip install torchmetrics
# !pip install torchinfo
# !pip install opencv-python
# !pip install ultralytics

"""Importing Libraries for Deep Learning and Data Processing"""

import os
import zipfile
import torch
from torchvision import datasets
from ultralytics import YOLO
from torchsummary import summary
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, roc_auc_score, auc
from sklearn.model_selection import train_test_split
import torch.nn as nn
import torchvision
from sklearn.metrics import accuracy_score, precision_recall_fscore_support
from sklearn.metrics import confusion_matrix
import seaborn as sns
from torchvision import transforms
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader
from torch import flatten
import torch.optim as optim
import os
import torchvision.transforms as transforms
import seaborn as sns
import torch.optim as optim
from torchmetrics import Accuracy
import time
from torch.nn import TransformerEncoder, TransformerEncoderLayer
import torchvision
from torch.utils.data import DataLoader, Subset
import matplotlib.pyplot as plt
import cv2
import shutil
from torch.utils.data import random_split

"""Change working Directory"""

def print_current_directory():
    print("Current Directory:", os.getcwd())

def change_directory(new_directory):
    try:
        os.chdir(new_directory)
        print("Updated Directory:", os.getcwd())
    except FileNotFoundError:
        print(f"Error: The directory '{new_directory}' does not exist.")


print_current_directory()
correct_directory = r'C:\Users\Ashraf\Work\UB\S3\CVIP\Project'
change_directory(correct_directory)

"""CUDA Functions"""

os.environ['CUDA_LAUNCH_BLOCKING'] = '1'
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

if device.type == "cuda":
    device_name = torch.cuda.get_device_name(0)
    print("Device used : ", device_name)
    device = "cuda"
else:
    print("Device unavailable")
    device = "cpu"

def clear_cuda_cache():
    torch.cuda.empty_cache()
    print("CUDA cache cleared.")

def print_cuda_memory_stats():
    allocated = torch.cuda.memory_allocated() / 1024**2
    cached = torch.cuda.memory_reserved() / 1024**2
    print(f"Memory Allocated: {allocated:.2f} MB")
    print(f"Memory Reserved (Cached): {cached:.2f} MB")

def free_cuda_memory():
    import gc
    gc.collect()
    torch.cuda.empty_cache()
    print("All CUDA memory freed.")

print_cuda_memory_stats()
clear_cuda_cache()

"""Checking and Extracting Dataset from ZIP File"""

def extract_zip_file(zip_path, extract_to='.'):
    if os.path.exists(zip_path):
        with zipfile.ZipFile(zip_path, 'r') as zip_file:
            zip_file.extractall(extract_to)
            print(f"Extracted contents of '{zip_path}' to '{extract_to}'.")
    else:
        print(f"File not found: {zip_path}")

dataset_path = './V2 - Traffic Signs.zip'
extract_zip_file(dataset_path)

"""Cropping and Saving Images Based on YOLOv11 Annotations"""

def crop_and_save_images(images_folder, labels_folder, op_folder):
    for image_file in os.listdir(images_folder):
        if image_file.endswith('.jpg'):
            process_image(image_file, images_folder, labels_folder, op_folder)

def process_image(image_file, images_folder, labels_folder, op_folder):
    image_path = os.path.join(images_folder, image_file)
    image = cv2.imread(image_path)
    if image is None:
        return

    annotation_file_path = os.path.join(labels_folder, os.path.splitext(image_file)[0] + '.txt')
    if not os.path.exists(annotation_file_path):
        print(f"Annotation file does not exist for: {image_file}")
        return

    annotations = read_annotations(annotation_file_path)
    for values in annotations:
        crop_and_save(image, values, image_file, op_folder)

def read_annotations(annotation_file_path):
    with open(annotation_file_path, 'r') as file:
        lines = file.readlines()
    return [line.strip().split() for line in lines]

def crop_and_save(image, annotation_values, image_file, op_folder):
    class_label = int(annotation_values[0])
    x, y, w, h = map(float, annotation_values[1:])

    x1 = int((x - w / 2) * image.shape[1])
    y1 = int((y - h / 2) * image.shape[0])
    x2 = int((x + w / 2) * image.shape[1])
    y2 = int((y + h / 2) * image.shape[0])

    roi = image[y1:y2, x1:x2]
    if roi.size == 0:
        return

    output_path = os.path.join(op_folder, str(class_label))
    os.makedirs(output_path, exist_ok=True)

    op_file = os.path.join(output_path, f'{os.path.splitext(image_file)[0]}_{class_label}.jpg')
    cv2.imwrite(op_file, roi)

train_images_folder = './V2 - Traffic Signs/train/images'
train_labels_folder = './V2 - Traffic Signs/train/labels'
train_op_folder = './cropped_images/train'

val_images_folder = './V2 - Traffic Signs/valid/images'
val_labels_folder = './V2 - Traffic Signs/valid/labels'
val_op_folder = './cropped_images/val'

crop_and_save_images(train_images_folder, train_labels_folder, train_op_folder)
crop_and_save_images(val_images_folder, val_labels_folder, val_op_folder)

"""Combining Train and Validation Folders into a Unified Dataset"""

def combine_folders(train_folder, val_folder, op_folder):
    os.makedirs(op_folder, exist_ok=True)

    class_names = sorted(set(os.listdir(train_folder)) | set(os.listdir(val_folder)))

    for class_name in class_names:
        class_output_path = os.path.join(op_folder, class_name)
        os.makedirs(class_output_path, exist_ok=True)

        train_class_path = os.path.join(train_folder, class_name)
        if os.path.exists(train_class_path):
            copy_files_from_folder(train_class_path, class_output_path)

        val_class_path = os.path.join(val_folder, class_name)
        if os.path.exists(val_class_path):
            copy_files_from_folder(val_class_path, class_output_path)

def copy_files_from_folder(src_folder, dest_folder):
    for filename in os.listdir(src_folder):
        shutil.copy(os.path.join(src_folder, filename), dest_folder)

train_folder = "./cropped_images/train"
val_folder = "./cropped_images/val"
op_folder = "./combined_data"

combine_folders(train_folder, val_folder, op_folder)

"""Reorganizing and Moving Files with Zero-Padded Directory Names

"""

def create_directories(base_path, num_dirs):
    os.makedirs(base_path, exist_ok=True)
    for i in range(num_dirs):
        dir_name = f"{i:02d}"
        os.makedirs(os.path.join(base_path, dir_name), exist_ok=True)

def move_files_to_padded_dirs(src_dir, dest_dir):
    for dir_name in os.listdir(src_dir):
        src_path = os.path.join(src_dir, dir_name)
        if os.path.isdir(src_path):
            new_dir_name = f"{int(dir_name):02d}" if int(dir_name) < 10 else dir_name
            dest_path = os.path.join(dest_dir, new_dir_name)

            os.makedirs(dest_path, exist_ok=True)

            for filename in os.listdir(src_path):
                shutil.move(os.path.join(src_path, filename), dest_path)

src_dir = "./combined_data"
op_dir = "./combined_data_new"

create_directories(op_dir, 55)

move_files_to_padded_dirs(src_dir, op_dir)

"""Transforming, Organizing, and Splitting Image Dataset for Training"""

def get_transform():
    return transforms.Compose([
        transforms.Resize((40, 40)),
        transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
    ])

def load_dataset(root_dir, transform):
    return datasets.ImageFolder(root=root_dir, transform=transform)

def get_unique_folders(dataset):
    class_names = dataset.classes
    unique_folders = set()
    for image_path, label in dataset.samples:
        folder_name = class_names[label]
        unique_folders.add((folder_name, label))
    return unique_folders

def print_unique_folders(unique_folders):
    for folder_name, label in unique_folders:
        print(f"Folder Name: {folder_name}, Label: {label}")

def split_dataset(dataset):
    dataset_len = [int(len(dataset) * 0.7), int(len(dataset) * 0.15)]
    dataset_len.append(len(dataset) - sum(dataset_len))
    return random_split(dataset, dataset_len)

transform = get_transform()
dataset = load_dataset(root_dir='combined_data_new', transform=transform)

train_dataset, val_dataset, test_dataset = split_dataset(dataset)

"""Using PyTorch DataLoader"""

def create_data_loader(dataset, batch_size=8, shuffle=True):
    return torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)

train_loader = create_data_loader(train_dataset, batch_size=32, shuffle=False)
test_loader = create_data_loader(test_dataset, batch_size=32, shuffle=False)
val_loader = create_data_loader(val_dataset, batch_size=32, shuffle=False)

"""Model Training Function with Training and Validation Steps"""

def train_one_epoch(model, train_loader, optimizer, criterion, device):
    model.train()
    total_length = 0
    total_training_loss = 0.0
    train_pred = []

    for data in train_loader:
        inputs, target = data
        inputs, target = inputs.to(device), target.to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        Prob = nn.Softmax(dim=1)(outputs)
        Y_pred = Prob.argmax(1).int()
        train_pred.append(accuracy(Y_pred, target).item())
        loss = criterion(outputs, target)
        loss.backward()
        optimizer.step()
        total_training_loss += loss.item()
        total_length += target.size(0)

    return total_training_loss / total_length, train_pred

def validate_model(model, val_loader, criterion, device):
    model.eval()
    total_length = 0
    total_validation_loss = 0.0
    vald_pred = []

    with torch.no_grad():
        for data in val_loader:
            inputs, target = data
            inputs, target = inputs.to(device), target.to(device)
            outputs = model(inputs)
            Prob = nn.Softmax(dim=1)(outputs)
            Y_pred = Prob.argmax(1).int()
            vald_pred.append(accuracy(Y_pred, target).item())
            val_loss = criterion(outputs, target)
            total_validation_loss += val_loss.item()
            total_length += target.size(0)

    return total_validation_loss / total_length, vald_pred

def log_metrics(epoch, total_training_loss, total_validation_loss, train_pred, vald_pred):
    training_acc_data.append(sum(train_pred) / len(train_pred))
    val_acc_data.append(sum(vald_pred) / len(vald_pred))
    training_loss_data.append(total_training_loss)
    val_loss_data.append(total_validation_loss)
    print(f'Epoch: {epoch+1}, Loss: {total_training_loss}, val_loss: {total_validation_loss}, train_acc: {training_acc_data[-1]}, val_acc : {val_acc_data[-1]}')

def train_model(model, epochs_no, train_loader, test_loader, val_loader, optimizer, criterion, device):
    for epoch in range(epochs_no):
        total_training_loss, train_pred = train_one_epoch(model, train_loader, optimizer, criterion, device)
        total_validation_loss, vald_pred = validate_model(model, val_loader, criterion, device)
        log_metrics(epoch, total_training_loss, total_validation_loss, train_pred, vald_pred)

    return training_loss_data, val_loss_data, training_acc_data, val_acc_data

"""VGG13 Model Implementation with Dynamic Feature Size Calculation"""

class VGG13(nn.Module):
    def __init__(self, num_classes=55):
        super(VGG13, self).__init__()
        self.features = self._make_features()
        conv_output_size = self._calculate_conv_output_size()
        self.classifier = self._make_classifier(conv_output_size, num_classes)

    def _make_features(self):
        return nn.Sequential(
            self._make_conv_block(3, 64, 2),
            self._make_conv_block(64, 128, 2),
            self._make_conv_block(128, 256, 2),
            self._make_conv_block(256, 512, 2),
            self._make_conv_block(512, 512, 2)
        )

    def _make_conv_block(self, in_channels, out_channels, num_convs):
        layers = [
            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=2, stride=2)
        ]
        return nn.Sequential(*layers)

    def _calculate_conv_output_size(self):
        test_tensor = torch.randn(1, 3, 40, 40)
        with torch.no_grad():
            features_output = self.features(test_tensor)
            return features_output.view(features_output.size(0), -1).size(1)

    def _make_classifier(self, conv_output_size, num_classes):
        return nn.Sequential(
            nn.Linear(conv_output_size, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(),
            nn.Linear(4096, 4096),
            nn.ReLU(inplace=True),
            nn.Dropout(),
            nn.Linear(4096, num_classes)
        )

    def forward(self, x):
        x = self.features(x)
        x = torch.flatten(x, 1)
        x = self.classifier(x)
        return x

"""Initializing and Training VGG13 Model"""

def setup_metrics(num_classes, device):
    return Accuracy(task="multiclass", num_classes=num_classes).to(device)

def initialize_training_lists():
    return [], [], [], []

def initialize_vgg_model(num_classes, device, learning_rate, weight_decay):
    model = VGG13(num_classes=num_classes).to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
    return model, criterion, optimizer

num_classes = 55
accuracy = setup_metrics(num_classes, device)
training_loss_data, val_loss_data, training_acc_data, val_acc_data = initialize_training_lists()

vgg, criterion, optimizer = initialize_vgg_model(55, device, 1e-5, 1e-9)

epoch_no = 10
training_loss_data, val_loss_data, training_acc_data, val_acc_data = train_model(
    # vit_resnet_model,
    vgg,
    epoch_no,
    train_loader,
    test_loader,
    val_loader,
    optimizer,
    criterion,
    device
)

"""Model Evaluation and Performance Metrics Calculation"""

def evaluate_model(model, test_loader, criterion, accuracy, device):
    testing_loss_data = []
    testing_acc_data = []
    testing_pred = []
    Y_test = []
    predictions = []
    total_testing_loss = 0
    total_length = 0

    model.to(device)
    model.eval()

    with torch.no_grad():
        for data in test_loader:
            inputs, target = data
            inputs, target = inputs.to(device), target.to(device)
            outputs = model(inputs)
            Prob = nn.Softmax(dim=1)(outputs)
            Y_pred = Prob.argmax(1).int()

            test_loss = criterion(outputs, target)
            total_testing_loss += test_loss.item()
            total_length += target.size(0)

            acc = accuracy(Y_pred, target).item()
            testing_pred.append(acc)
            Y_test.append(target)
            predictions.append(Y_pred)

    average_testing_loss = total_testing_loss / total_length
    testing_loss_data.append(average_testing_loss)
    testing_acc_data.append(sum(testing_pred) / len(testing_pred))

    return testing_loss_data, testing_acc_data, predictions, Y_test

def calculate_metrics(Y_pred, Y_test, accuracy):
    Final_accuracy = accuracy(Y_pred, Y_test).item()
    precision, recall, f1, _ = precision_recall_fscore_support(Y_test.cpu(), Y_pred.cpu())
    return Final_accuracy, precision, recall, f1

def print_performance_metrics(training_time, Final_accuracy, precision, recall, f1):
    print("Performance Metrics:")
    print(f"Time to Train : {training_time} seconds")
    print(f"Accuracy      : {Final_accuracy * 100:.2f} %")
    print(f"Precision     : {precision}")
    print(f"Recall        : {recall}")
    print(f"F1 Score      : {f1}")

start_time = time.time()

testing_loss_data, testing_acc_data, predictions, Y_test = evaluate_model(vgg, test_loader, criterion, accuracy, device)

Y_pred = torch.cat([p for p in predictions])
Y_test = torch.cat([t for t in Y_test])
Final_accuracy, precision, recall, f1 = calculate_metrics(Y_pred, Y_test, accuracy)
training_time = time.time() - start_time

print_performance_metrics(training_time, Final_accuracy, precision, recall, f1)

"""Printing Final Accuracy and Loss Metrics"""

def calculate_average_loss(loss_data):
    return sum(loss_data) / len(loss_data)

def print_accuracy_and_loss(training_acc_data, val_acc_data, testing_acc_data,
                            training_loss_data, val_loss_data, testing_loss_data):
    print("Accuracy and Loss values:")
    print()
    print(f"Training Accuracy : {training_acc_data[-1]*100:.2f} %")
    print(f"Validation Accuracy : {val_acc_data[-1]*100:.2f} %")
    print(f"Testing Accuracy : {testing_acc_data[-1]*100:.2f} %")
    print()
    print(f"Training Loss : {calculate_average_loss(training_loss_data)}")
    print(f"Validation Loss : {calculate_average_loss(val_loss_data)}")
    print(f"Testing Loss : {calculate_average_loss(testing_loss_data)}")

print_accuracy_and_loss(training_acc_data, val_acc_data, testing_acc_data,
                        training_loss_data, val_loss_data, testing_loss_data)

"""Plotting Training and Validation Metrics Over Epochs"""

def plot_training_metrics(epochs, training_acc_data, val_acc_data, training_loss_data, val_loss_data):
    def plot_accuracy():
        plt.figure(figsize=(10, 6))
        plt.plot(epochs, training_acc_data, label='Training Data')
        plt.plot(epochs, val_acc_data, label='Validation Data')
        plt.xlabel('Epochs')
        plt.ylabel('Accuracy')
        plt.title('Training and Validation Accuracy Data')
        plt.legend()
        plt.show()

    def plot_loss():
        plt.figure(figsize=(10, 6))
        plt.plot(epochs, training_loss_data, label='Training Data')
        plt.plot(epochs, val_loss_data, label='Validation Data')
        plt.xlabel('Epochs')
        plt.ylabel('Loss')
        plt.title('Training and Validation Loss Data')
        plt.legend()

    def plot_confusion_matrix(Y_test, Y_pred):
        cmat = confusion_matrix(Y_test.cpu(), Y_pred.cpu())
        plt.figure(figsize=(14, 12))
        sns.heatmap(cmat, annot=True, fmt="d")
        plt.xlabel('Predicted Labels')
        plt.ylabel('True Labels')
        plt.title('Confusion Matrix')
        plt.show()

    plot_accuracy()
    plot_loss()
    plot_confusion_matrix(Y_test, Y_pred)

epochs = range(1, epoch_no+1)
plot_training_metrics(epochs, training_acc_data, val_acc_data, training_loss_data, val_loss_data)

"""Saving Trained Model Weights"""

def save_model(model, filepath="name.pth"):
    torch.save(model.state_dict(), filepath)

save_model(vgg,"vgg.pth")

"""ResNet34 Model Implementation with Residual Blocks"""

class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1, downsample=None):
        super(ResidualBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.downsample = downsample

    def forward(self, x):
        identity = x
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        if self.downsample is not None:
            identity = self.downsample(x)
        out += identity
        out = self.relu(out)
        return out

class ResNet34(nn.Module):
    def __init__(self, num_classes=55):
        super(ResNet34, self).__init__()
        self.block1 = self._make_initial_block()
        self.block2 = self._make_layer(64, 64, 3)
        self.block3 = self._make_layer(64, 128, 4, stride=2)
        self.block4 = self._make_layer(128, 256, 6, stride=2)
        self.block5 = self._make_layer(256, 512, 3, stride=2)
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512, num_classes)

    def _make_initial_block(self):
        return nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        )

    def _make_layer(self, in_channels, out_channels, blocks, stride=1):
        downsample = None
        if stride != 1 or in_channels != out_channels:
            downsample = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )
        layers = [ResidualBlock(in_channels, out_channels, stride, downsample)]
        layers.extend([ResidualBlock(out_channels, out_channels) for _ in range(1, blocks)])
        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.block1(x)
        x = self.block2(x)
        x = self.block3(x)
        x = self.block4(x)
        x = self.block5(x)
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.fc(x)
        return x

"""Initializing and Training Resnet Model"""

def initialize_resnet_model(num_classes, device, learning_rate=1e-6, weight_decay=1e-9):
    model = ResNet34(num_classes=num_classes).to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
    return model, criterion, optimizer

num_classes = 55
accuracy = setup_metrics(num_classes, device)
training_loss_data, val_loss_data, training_acc_data, val_acc_data = initialize_training_lists()

resnet, criterion, optimizer = initialize_resnet_model(55, device)

epoch_no = 10
training_loss_data, val_loss_data, training_acc_data, val_acc_data = train_model(
    resnet,
    epoch_no,
    train_loader,
    test_loader,
    val_loader,
    optimizer,
    criterion,
    device
)

"""Model Evaluation and Performance Metrics Calculation"""

start_time = time.time()

testing_loss_data, testing_acc_data, predictions, Y_test = evaluate_model(resnet, test_loader, criterion, accuracy, device)

Y_pred = torch.cat([p for p in predictions])
Y_test = torch.cat([t for t in Y_test])
Final_accuracy, precision, recall, f1 = calculate_metrics(Y_pred, Y_test, accuracy)
training_time = time.time() - start_time

print_performance_metrics(training_time, Final_accuracy, precision, recall, f1)

"""Printing Final Accuracy and Loss Metrics"""

print_accuracy_and_loss(training_acc_data, val_acc_data, testing_acc_data,
                        training_loss_data, val_loss_data, testing_loss_data)

"""Plotting Training and Validation Metrics Over Epochs"""

epochs = range(1, epoch_no+1)
plot_training_metrics(epochs, training_acc_data, val_acc_data, training_loss_data, val_loss_data)

"""Saving Trained Model Weights"""

save_model(resnet,"resnet.pth")

"""Vision Transformer Model Implementation"""

class VisionTransformer_Model(nn.Module):
    def __init__(self, in_channels, img_size, num_classes, embedding_dim, num_attention_heads, patch_size, num_layers):
        super(VisionTransformer_Model, self).__init__()
        self.patch_embedding = self._create_patch_embedding(in_channels, embedding_dim, patch_size)
        self.encoder = self._create_transformer_encoder(embedding_dim, num_attention_heads, num_layers)
        self.op = nn.Linear(embedding_dim, num_classes)

    def _create_patch_embedding(self, in_channels, embedding_dim, patch_size):
        return nn.Sequential(
            nn.Conv2d(in_channels, embedding_dim, kernel_size=patch_size, stride=patch_size),
            nn.BatchNorm2d(embedding_dim),
            nn.Flatten(2)
        )

    def _create_transformer_encoder(self, embedding_dim, num_attention_heads, num_layers):
        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=num_attention_heads)
        return nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

    def forward(self, x):
        x = self.patch_embedding(x)
        x = self.encoder(x)
        x = x.mean(dim=1)
        x = self.op(x)
        return x

"""Initializing Vision Transformer Model"""

def initialize_vision_transformer(in_channels, img_size, num_classes, embedding_dim, num_attention_heads, patch_size, num_layers, device):
    model = VisionTransformer_Model(
        in_channels=in_channels,
        img_size=img_size,
        num_classes=num_classes,
        embedding_dim=embedding_dim,
        num_attention_heads=num_attention_heads,
        patch_size=patch_size,
        num_layers=num_layers
    ).to(device)
    return model

in_channels = 3
img_size = 40
num_classes = 55
embedding_dim = 13 * 13
num_attention_heads = 13
patch_size = 3
num_layers = 12

vision_transformer = initialize_vision_transformer(
    in_channels,
    img_size,
    num_classes,
    embedding_dim,
    num_attention_heads,
    patch_size,
    num_layers,
    device
)

"""Initializing and Training Vision Transformer Model"""

def initialize_vision_transformer_model(model, device, learning_rate=1e-5, weight_decay=1e-9):
    model = model.to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
    return model, criterion, optimizer

num_classes = 55
accuracy = setup_metrics(num_classes, device)
vision_transformer, criterion, optimizer = initialize_vision_transformer_model(vision_transformer, device)

epoch_no = 10
training_loss_data, val_loss_data, training_acc_data, val_acc_data = train_model(
    vision_transformer,
    epoch_no,
    train_loader,
    test_loader,
    val_loader,
    optimizer,
    criterion,
    device
)

"""Model Evaluation and Performance Metrics Calculation"""

start_time = time.time()

testing_loss_data, testing_acc_data, predictions, Y_test = evaluate_model(vision_transformer, test_loader, criterion, accuracy, device)

Y_pred = torch.cat([p for p in predictions])
Y_test = torch.cat([t for t in Y_test])
Final_accuracy, precision, recall, f1 = calculate_metrics(Y_pred, Y_test, accuracy)
training_time = time.time() - start_time

print_performance_metrics(training_time, Final_accuracy, precision, recall, f1)

"""Printing Final Accuracy and Loss Metrics"""

print_accuracy_and_loss(training_acc_data, val_acc_data, testing_acc_data,
                        training_loss_data, val_loss_data, testing_loss_data)

"""Plotting Training and Validation Metrics Over Epochs"""

def plot_accuracy_and_loss(epochs, training_acc_data, val_acc_data, training_loss_data, val_loss_data):
    num_epochs = len(epochs)
    training_acc_data = training_acc_data[:num_epochs]
    val_acc_data = val_acc_data[:num_epochs]
    training_loss_data = training_loss_data[:num_epochs]
    val_loss_data = val_loss_data[:num_epochs]

    plot_training_metrics(epochs, training_acc_data, val_acc_data, training_loss_data, val_loss_data)

epochs = range(1, epoch_no + 1)
plot_accuracy_and_loss(epochs, training_acc_data, val_acc_data, training_loss_data, val_loss_data)

"""Saving Trained Model Weights"""

save_model(vision_transformer,"vision_transformer.pth")

free_cuda_memory()

"""VIT ResNet Hybrid Model

Transforming, Organizing, and Splitting Image Dataset for Training
"""

transform = get_transform()
dataset = load_dataset(root_dir='combined_data_new', transform=transform)

unique_folders = get_unique_folders(dataset)
print_unique_folders(unique_folders)

train_dataset, val_dataset, test_dataset = split_dataset(dataset)

"""Dataset Analysis"""

def print_dataset_info(dataset):
    print("Total Number of images:", len(dataset))
    print("Classes in the dataset:", dataset.classes)

print_dataset_info(dataset)

"""Visualizing a Batch of Images with Class Labels"""

def get_labels_map():
    return {
        0: 'forb_ahead', 1: 'forb_left', 2: 'forb_overtake', 3: 'forb_right',
        4: 'forb_speed_over_10', 5: 'forb_speed_over_100', 6: 'forb_speed_over_130',
        7: 'forb_speed_over_20', 8: 'forb_speed_over_30', 9: 'forb_speed_over_40',
        10: 'forb_speed_over_5', 11: 'forb_speed_over_50', 12: 'forb_speed_over_60',
        13: 'forb_speed_over_70', 14: 'forb_speed_over_80', 15: 'forb_speed_over_90',
        16: 'forb_stopping', 17: 'forb_trucks', 18: 'forb_u_turn',
        19: 'forb_weight_over_3.5t', 20: 'forb_weight_over_7.5t',
        21: 'info_bus_station', 22: 'info_crosswalk', 23: 'info_highway',
        24: 'info_one_way_traffic', 25: 'info_parking', 26: 'info_taxi_parking',
        27: 'mand_bike_lane', 28: 'mand_left', 29: 'mand_left_right',
        30: 'mand_pass_left', 31: 'mand_pass_left_right', 32: 'mand_pass_right',
        33: 'mand_right', 34: 'mand_roundabout', 35: 'mand_straigh_left',
        36: 'mand_straight', 37: 'mand_straight_right',
        38: 'prio_give_way', 39: 'prio_priority_road',
        40: 'prio_stop',41:'warn_children',
        42:'warn_construction',
        43:'warn_crosswalk',
        44:'warn_cyclists',
        45:'warn_domestic_animals',
        46:'warn_other_dangers',
        47:'warn_poor_road_surface',
        48:'warn_roundabout',
        49:'warn_slippery_road',
        50:'warn_speed_bumper',
        51:'warn_traffic_light',
        52:'warn_tram',
        53:'warn_two_way_traffic',
        54:'warn_wild_animals'
    }

def plot_images(images, labels, labels_map):
    fig, ax = plt.subplots(1, len(images), figsize=(10,3))
    for i, img in enumerate(images):
        img_np = img.permute(1,2,0).numpy()
        index = labels[i].item()
        ax[i].imshow(img_np.squeeze(), cmap="gray")
        ax[i].set_title(labels_map[index])
        ax[i].axis("off")
    plt.tight_layout()
    plt.show()

data_loader = create_data_loader(dataset)
images, labels = next(iter(data_loader))
labels_map = get_labels_map()
plot_images(images, labels, labels_map)

"""Counting and Visualizing the Number of Samples per Class"""

def count_class_samples(dataset):
    class_counts = {}
    for _, label in dataset:
        if label not in class_counts:
            class_counts[label] = 0
        class_counts[label] += 1
    return class_counts

def plot_class_distribution(class_counts):
    labels = list(class_counts.keys())
    counts = list(class_counts.values())
    plt.figure(figsize=(15, 10))
    plt.bar(labels, counts)
    plt.xlabel('Class')
    plt.ylabel('Number of Samples')
    plt.title('Number of Samples in Each Class')
    plt.xticks(np.arange(len(labels)), labels)
    plt.show()

class_counts = count_class_samples(dataset)
plot_class_distribution(class_counts)

"""Creating Data Loaders for Training, Validation, and Test Datasets"""

train_loader = create_data_loader(train_dataset, batch_size=32, shuffle=False)
test_loader = create_data_loader(test_dataset, batch_size=32, shuffle=False)
val_loader = create_data_loader(val_dataset, batch_size=32, shuffle=False)

"""Dataset Analysis: Counting Images in Training, Validation, and Testing Sets"""

def print_dataset_sizes(train_dataset, val_dataset, test_dataset):
    print("Total Number of images in Training Data:", len(train_dataset))
    print("Total Number of images in Validation Data:", len(val_dataset))
    print("Total Number of images in Testing Data:", len(test_dataset))

print_dataset_sizes(train_dataset, val_dataset, test_dataset)

"""Defining and Initializing Combined VisionTransformer-ResNet Model"""

class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride=1, downsample=None):
        super(ResidualBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.downsample = downsample

    def forward(self, x):
        identity = x
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)
        out = self.conv2(out)
        out = self.bn2(out)
        if self.downsample is not None:
            identity = self.downsample(x)
        out += identity
        out = self.relu(out)
        return out

class ResNet34(nn.Module):
    def __init__(self, num_classes=169):
        super(ResNet34, self).__init__()
        self.block1 = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        )
        self.block2 = self._make_layer(64, 64, 3)
        self.block3 = self._make_layer(64, 128, 4, stride=2)
        self.block4 = self._make_layer(128, 256, 6, stride=2)
        self.block5 = self._make_layer(256, 512, 3, stride=2)
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
        self.fc = nn.Linear(512, num_classes)

    def _make_layer(self, in_channels, out_channels, blocks, stride=1):
        downsample = None
        if stride != 1 or in_channels != out_channels:
            downsample = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(out_channels)
            )
        layers = []
        layers.append(ResidualBlock(in_channels, out_channels, stride, downsample))
        for _ in range(1, blocks):
            layers.append(ResidualBlock(out_channels, out_channels))
        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.block1(x)
        x = self.block2(x)
        x = self.block3(x)
        x = self.block4(x)
        x = self.block5(x)
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        x = self.fc(x)
        return x

class VisionTransformer_Model(nn.Module):
    def __init__(self, in_channels, img_size, embedding_dim, num_attention_heads, patch_size, num_layers):
        super(VisionTransformer_Model, self).__init__()
        self.patch_embedding = nn.Sequential(
            nn.Conv2d(in_channels, embedding_dim, kernel_size=patch_size, stride=patch_size),
            nn.BatchNorm2d(embedding_dim),
            nn.Flatten(2)
        )
        self.encoder = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=num_attention_heads), num_layers=num_layers
        )

    def forward(self, x):
        x = self.patch_embedding(x)
        x = self.encoder(x)
        x = x.mean(dim=1)
        return x

class VisionTransformer_ResNet_Concatenator(nn.Module):
    def __init__(self, vision_model, resnet_model, conv1d_out_channels, num_classes):
        super(VisionTransformer_ResNet_Concatenator, self).__init__()
        self.vision_model = vision_model
        self.resnet_model = resnet_model
        self.conv1d_out_channels = conv1d_out_channels
        with torch.no_grad():
            resnet_output_dim = resnet_model(torch.zeros(1, 3, 40, 40).to(device)).shape[1]
            vit_output_dim = vision_model(torch.zeros(1, 3, 40, 40).to(device)).shape[1]
        self.concat_dim = vit_output_dim + resnet_output_dim
        self.conv1d = nn.Conv1d(self.concat_dim, conv1d_out_channels, kernel_size=1)
        self.fc = nn.Linear(conv1d_out_channels, num_classes)

    def forward(self, x):
        vit_output = self.vision_model(x)
        resnet_output = self.resnet_model(x)
        concatenated_output = torch.cat((vit_output, resnet_output), dim=1)
        concatenated_output = concatenated_output.unsqueeze(2)
        concatenated_output = concatenated_output.permute(0, 1, 2)
        conv1d_output = self.conv1d(concatenated_output)
        conv1d_output = conv1d_output.squeeze(2)
        output = self.fc(conv1d_output)
        return output

def initialize_models(device):
    resnet_model_con = ResNet34().to(device)
    in_channels, img_size, embedding_dim_vision = 3, 40, 13 * 13
    num_attention_heads, patch_size, num_layers = 13, 3, 12
    vit_model_con = VisionTransformer_Model(in_channels, img_size, embedding_dim_vision, num_attention_heads, patch_size, num_layers).to(device)
    return resnet_model_con, vit_model_con

resnet_model_con, vit_model_con = initialize_models(device)
conv1d_out_channels = 128
num_classes = 55
vit_resnet_model = VisionTransformer_ResNet_Concatenator(vit_model_con, resnet_model_con, conv1d_out_channels, num_classes).to(device)
print(vit_resnet_model)

""" Model Training Components and Training Configuration"""

def setup_model_components(vit_model_con, resnet_model_con, conv1d_out_channels, num_classes, device, learning_rate=1e-4, weight_decay=1e-5):
    model = VisionTransformer_ResNet_Concatenator(vit_model_con, resnet_model_con, conv1d_out_channels, num_classes).to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
    return model, criterion, optimizer

num_classes = 55
accuracy = setup_metrics(num_classes, device)
training_loss_data, val_loss_data, training_acc_data, val_acc_data = initialize_training_lists()

vit_resnet_model, criterion, optimizer = setup_model_components(
    vit_model_con,
    resnet_model_con,
    conv1d_out_channels,
    num_classes,
    device
)

epoch_no = 20
training_loss_data, val_loss_data, training_acc_data, val_acc_data = train_model(
    vit_resnet_model,
    epoch_no,
    train_loader,
    test_loader,
    val_loader,
    optimizer,
    criterion,
    device
)

"""Model Evaluation and Performance Metrics Calculation"""

start_time = time.time()

testing_loss_data, testing_acc_data, predictions, Y_test = evaluate_model(vit_resnet_model, test_loader, criterion, accuracy, device)

Y_pred = torch.cat([p for p in predictions])
Y_test = torch.cat([t for t in Y_test])
Final_accuracy, precision, recall, f1 = calculate_metrics(Y_pred, Y_test, accuracy)
training_time = time.time() - start_time

print_performance_metrics(training_time, Final_accuracy, precision, recall, f1)

"""Printing Final Accuracy and Loss Metrics"""

print_accuracy_and_loss(training_acc_data, val_acc_data, testing_acc_data,
                        training_loss_data, val_loss_data, testing_loss_data)

"""Plotting Training and Validation Metrics Over Epochs"""

epochs = range(1, epoch_no+1)
plot_training_metrics(epochs, training_acc_data, val_acc_data, training_loss_data, val_loss_data)

"""Saving Trained Model Weights"""

save_model(vit_resnet_model,"vit_resnet_modelyolo.pth")